# ğŸ—ï¸ Baidu-Tieba-Community-Topic-Data-Asset-Construction-Lightweight-Version

**ç™¾åº¦è´´å§ç¤¾åŒºä¸»é¢˜æ•°æ®èµ„äº§å»ºè®¾ï¼ˆè½»é‡ç‰ˆï¼‰**

---

## ğŸ§© ä¸€ã€é¡¹ç›®æ€»ä½“ç»“æ„ä¸ç¯å¢ƒè§„åˆ’

### ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„ï¼ˆå»ºè®®ç›´æ¥æ–°å»ºå¦‚ä¸‹æ–‡ä»¶å¤¹ï¼‰
```bash
/root/baidu_tieba_dw/
â”‚
â”œâ”€â”€ data/                      # æ¨¡æ‹Ÿæºæ•°æ®ï¼ˆPythonç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ gen_data.py
â”‚   â”œâ”€â”€ user_info.csv
â”‚   â”œâ”€â”€ post_info.csv
â”‚   â””â”€â”€ comment_info.csv
â”‚
â”œâ”€â”€ scripts/                   # æ•°æ®å¯¼å…¥ä¸Hiveè„šæœ¬
â”‚   â”œâ”€â”€ load_to_hdfs.sh
â”‚   â”œâ”€â”€ hive_create_all.sql
â”‚   â”œâ”€â”€ hive_dwd_clean.sql
â”‚   â”œâ”€â”€ hive_dws_summary.sql
â”‚   â”œâ”€â”€ hive_ads_analysis.sql
â”‚
â””â”€â”€ README.txt

ğŸ“Œ åˆ›å»ºå‘½ä»¤
mkdir -p ~/baidu_tieba_dw/{data,scripts}
cd ~/baidu_tieba_dw

âš™ï¸ äºŒã€ç¯å¢ƒå‡†å¤‡
é¡¹ç›®	è¦æ±‚	æ£€æŸ¥å‘½ä»¤
Hadoop	å·²å®‰è£…å¹¶è¿è¡Œ	hadoop version
HDFS	å·²å¯åŠ¨	hdfs dfs -ls /
Hive	å·²é…ç½®å…ƒæ•°æ®åº“å¹¶å¯åŠ¨	hive --version
Python	3.7+	python3 --version
ğŸ§± ä¸‰ã€æ•°æ®ç”Ÿæˆï¼ˆdata/gen_data.pyï¼‰

æ–‡ä»¶è·¯å¾„ï¼š ~/baidu_tieba_dw/data/gen_data.py

import random, csv
from datetime import datetime, timedelta

users = [f"user_{i}" for i in range(1, 51)]
forums = ["AIå§", "ç¼–ç¨‹å§", "æ¸¸æˆå§", "è€ƒç ”å§"]

# --- ç”¨æˆ·è¡¨ ---
with open("user_info.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["user_id", "register_time", "user_level"])
    for u in users:
        writer.writerow([u, (datetime.now() - timedelta(days=random.randint(0, 600))).strftime("%Y-%m-%d"), random.randint(1, 10)])

# --- å¸–å­è¡¨ ---
with open("post_info.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["post_id", "title", "user_id", "forum_name", "create_time", "reply_count", "like_count"])
    for i in range(1, 501):
        writer.writerow([
            i,
            f"å¸–å­æ ‡é¢˜_{i}",
            random.choice(users),
            random.choice(forums),
            (datetime.now() - timedelta(days=random.randint(0, 90))).strftime("%Y-%m-%d %H:%M:%S"),
            random.randint(0, 100),
            random.randint(0, 500)
        ])

# --- è¯„è®ºè¡¨ ---
with open("comment_info.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["comment_id", "post_id", "user_id", "comment_content", "comment_time"])
    for i in range(1, 1001):
        writer.writerow([
            i,
            random.randint(1, 500),
            random.choice(users),
            random.choice(["ä¸é”™", "æ”¯æŒ", "å­¦ä¹ äº†", "é¡¶", "æ°´è´´", "å¹¿å‘Š", "å“ˆå“ˆ"]),
            (datetime.now() - timedelta(days=random.randint(0, 90))).strftime("%Y-%m-%d %H:%M:%S")
        ])


ğŸ“Œ è¿è¡Œå‘½ä»¤

cd ~/baidu_tieba_dw/data
python3 gen_data.py


âœ… ç”Ÿæˆæ–‡ä»¶

user_info.csv
post_info.csv
comment_info.csv

ğŸšš å››ã€æ•°æ®å¯¼å…¥ï¼ˆscripts/load_to_hdfs.shï¼‰

æ–‡ä»¶è·¯å¾„ï¼š ~/baidu_tieba_dw/scripts/load_to_hdfs.sh

#!/bin/bash

# åˆ›å»ºHDFSç›®å½•
hdfs dfs -mkdir -p /data/ods/baidu_tieba/user_info
hdfs dfs -mkdir -p /data/ods/baidu_tieba/post_info
hdfs dfs -mkdir -p /data/ods/baidu_tieba/comment_info

# ä¸Šä¼ æ•°æ®
hdfs dfs -put -f ../data/user_info.csv /data/ods/baidu_tieba/user_info/
hdfs dfs -put -f ../data/post_info.csv /data/ods/baidu_tieba/post_info/
hdfs dfs -put -f ../data/comment_info.csv /data/ods/baidu_tieba/comment_info/


ğŸ“Œ è¿è¡Œå‘½ä»¤

cd ~/baidu_tieba_dw/scripts
bash load_to_hdfs.sh


âœ… æ£€æŸ¥æ˜¯å¦ä¸Šä¼ æˆåŠŸ

hdfs dfs -ls /data/ods/baidu_tieba/user_info/

ğŸ§® äº”ã€ODS å±‚ï¼ˆHive åŸå§‹å±‚ï¼‰

æ–‡ä»¶è·¯å¾„ï¼š ~/baidu_tieba_dw/scripts/hive_create_all.sql

-- ç”¨æˆ·è¡¨
CREATE EXTERNAL TABLE IF NOT EXISTS ods_user_info (
  user_id STRING,
  register_time STRING,
  user_level INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/ods/baidu_tieba/user_info/';

-- å¸–å­è¡¨
CREATE EXTERNAL TABLE IF NOT EXISTS ods_post_info (
  post_id INT,
  title STRING,
  user_id STRING,
  forum_name STRING,
  create_time STRING,
  reply_count INT,
  like_count INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/ods/baidu_tieba/post_info/';

-- è¯„è®ºè¡¨
CREATE EXTERNAL TABLE IF NOT EXISTS ods_comment_info (
  comment_id INT,
  post_id INT,
  user_id STRING,
  comment_content STRING,
  comment_time STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/ods/baidu_tieba/comment_info/';


ğŸ“Œ æ‰§è¡Œå‘½ä»¤

hive -f ~/baidu_tieba_dw/scripts/hive_create_all.sql


âœ… æ£€æŸ¥

SHOW TABLES LIKE 'ods_%';
SELECT * FROM ods_post_info LIMIT 5;

ğŸ§¹ å…­ã€DWD æ¸…æ´—å±‚

æ–‡ä»¶è·¯å¾„ï¼š ~/baidu_tieba_dw/scripts/hive_dwd_clean.sql

-- æ¸…æ´—å¸–å­è¡¨
CREATE TABLE dwd_post_detail AS
SELECT
  post_id,
  title,
  user_id,
  forum_name,
  from_unixtime(unix_timestamp(create_time, 'yyyy-MM-dd HH:mm:ss')) AS create_time,
  IF(reply_count < 0, 0, reply_count) AS reply_count,
  IF(like_count < 0, 0, like_count) AS like_count
FROM ods_post_info
WHERE title IS NOT NULL AND user_id IS NOT NULL;

-- æ¸…æ´—è¯„è®ºè¡¨ï¼ˆå»é™¤å¹¿å‘Šã€æ°´è´´ï¼‰
CREATE TABLE dwd_comment_detail AS
SELECT
  comment_id,
  post_id,
  user_id,
  comment_content,
  from_unixtime(unix_timestamp(comment_time, 'yyyy-MM-dd HH:mm:ss')) AS comment_time
FROM ods_comment_info
WHERE comment_content NOT RLIKE 'å¹¿å‘Š|æ°´è´´|æ”¿æ²»|è¿æ³•';

-- æ¸…æ´—ç”¨æˆ·è¡¨
CREATE TABLE dwd_user_info AS
SELECT
  user_id,
  from_unixtime(unix_timestamp(register_time, 'yyyy-MM-dd')) AS register_time,
  user_level
FROM ods_user_info
WHERE user_id IS NOT NULL;


ğŸ“Œ æ‰§è¡Œå‘½ä»¤

hive -f ~/baidu_tieba_dw/scripts/hive_dwd_clean.sql


âœ… æ£€æŸ¥

SHOW TABLES LIKE 'dwd_%';
SELECT COUNT(*) FROM dwd_comment_detail;

ğŸ“Š ä¸ƒã€DWS æ±‡æ€»å±‚

æ–‡ä»¶è·¯å¾„ï¼š ~/baidu_tieba_dw/scripts/hive_dws_summary.sql

-- ç”¨æˆ·æ´»è·ƒæŒ‡æ ‡
CREATE TABLE dws_user_active AS
SELECT
  u.user_id,
  COUNT(DISTINCT p.post_id) AS post_count,
  SUM(p.reply_count) AS total_replies,
  SUM(p.like_count) AS total_likes
FROM dwd_user_info u
LEFT JOIN dwd_post_detail p ON u.user_id = p.user_id
GROUP BY u.user_id;

-- è´´å§çƒ­åº¦æŒ‡æ ‡
CREATE TABLE dws_forum_hot_rank AS
SELECT
  forum_name,
  COUNT(post_id) AS post_num,
  SUM(reply_count + like_count) AS activity_score
FROM dwd_post_detail
GROUP BY forum_name;

-- è¿‘30å¤©å‘å¸–è¶‹åŠ¿
CREATE TABLE dws_post_trend AS
SELECT
  date_format(create_time,'yyyy-MM-dd') AS day,
  COUNT(*) AS post_count
FROM dwd_post_detail
WHERE create_time >= date_sub(current_date(),30)
GROUP BY date_format(create_time,'yyyy-MM-dd');


ğŸ“Œ æ‰§è¡Œå‘½ä»¤

hive -f ~/baidu_tieba_dw/scripts/hive_dws_summary.sql


âœ… æ£€æŸ¥

SELECT * FROM dws_forum_hot_rank;

ğŸ§  å…«ã€ADS åº”ç”¨å±‚ï¼ˆåˆ†æç»“æœï¼‰

æ–‡ä»¶è·¯å¾„ï¼š ~/baidu_tieba_dw/scripts/hive_ads_analysis.sql

-- ç”¨æˆ·åˆ†å±‚æ ‡ç­¾
CREATE TABLE ads_user_tag AS
SELECT
  user_id,
  CASE 
    WHEN total_replies + total_likes > 400 THEN 'é«˜æ´»è·ƒ'
    WHEN total_replies + total_likes BETWEEN 100 AND 400 THEN 'ä¸­æ´»è·ƒ'
    ELSE 'ä½æ´»è·ƒ'
  END AS user_tag
FROM dws_user_active;

-- çƒ­å¸–æ¦œTop10
CREATE TABLE ads_hot_post AS
SELECT
  post_id,
  title,
  user_id,
  forum_name,
  (reply_count + like_count) AS hot_score
FROM dwd_post_detail
ORDER BY hot_score DESC
LIMIT 10;

-- å„è´´å§ç»¼åˆæ’å
CREATE TABLE ads_forum_rank AS
SELECT
  forum_name,
  post_num,
  activity_score,
  RANK() OVER (ORDER BY activity_score DESC) AS rank_id
FROM dws_forum_hot_rank;


ğŸ“Œ æ‰§è¡Œå‘½ä»¤

hive -f ~/baidu_tieba_dw/scripts/hive_ads_analysis.sql


âœ… æ£€æŸ¥

SELECT * FROM ads_user_tag LIMIT 10;
SELECT * FROM ads_hot_post LIMIT 10;
SELECT * FROM ads_forum_rank LIMIT 10;

ğŸ“¤ ä¹ã€ç»“æœå¯¼å‡ºï¼ˆå¯é€‰ï¼‰
hive -e "SELECT * FROM ads_hot_post;" > ~/baidu_tieba_dw/hot_post_result.csv

ğŸ§­ åã€æ‰§è¡Œé¡ºåºä¸€è§ˆè¡¨
é˜¶æ®µ	æ–‡ä»¶/å‘½ä»¤	è¯´æ˜
1ï¸âƒ£ æ•°æ®ç”Ÿæˆ	python3 gen_data.py	ç”ŸæˆCSVæ•°æ®
2ï¸âƒ£ ä¸Šä¼ åˆ°HDFS	bash load_to_hdfs.sh	å¯¼å…¥HDFS
3ï¸âƒ£ å»ºODSè¡¨	hive -f hive_create_all.sql	å¤–éƒ¨è¡¨æ˜ å°„åŸå§‹æ•°æ®
4ï¸âƒ£ æ¸…æ´—åˆ°DWD	hive -f hive_dwd_clean.sql	æ¸…æ´—ã€å»å™ªã€è§„èŒƒåŒ–
5ï¸âƒ£ æ±‡æ€»åˆ°DWS	hive -f hive_dws_summary.sql	æ±‡æ€»å±‚æŒ‡æ ‡è®¡ç®—
6ï¸âƒ£ è¾“å‡ºåˆ°ADS	hive -f hive_ads_analysis.sql	ç”¨æˆ·æ ‡ç­¾ã€çƒ­å¸–æ¦œã€è´´å§æ’è¡Œ
7ï¸âƒ£ å¯¼å‡ºç»“æœ	hive -e ... > xxx.csv	è¾“å‡ºä¸ºCSV
ğŸ§¾ åä¸€ã€README.txt å»ºè®®å†…å®¹
é¡¹ç›®åç§°ï¼šç™¾åº¦è´´å§ç¤¾åŒºä¸»é¢˜æ•°æ®èµ„äº§å»ºè®¾ï¼ˆè½»é‡ç‰ˆï¼‰
ç¯å¢ƒè¦æ±‚ï¼šHadoop 3.x + Hive 3.x + Python 3.7+

æ‰§è¡Œé¡ºåºï¼š
1. cd data && python3 gen_data.py
2. cd scripts && bash load_to_hdfs.sh
3. hive -f hive_create_all.sql
4. hive -f hive_dwd_clean.sql
5. hive -f hive_dws_summary.sql
6. hive -f hive_ads_analysis.sql
7. hive -e "SELECT * FROM ads_hot_post;" > hot_post_result.csv

æœ€ç»ˆè¾“å‡ºç»“æœï¼š
- ç”¨æˆ·åˆ†å±‚æ ‡ç­¾ï¼šads_user_tag
- çƒ­å¸–æ¦œTop10ï¼šads_hot_post
- è´´å§æ´»è·ƒåº¦æ’è¡Œï¼šads_forum_rank

ğŸ’¡ åäºŒã€é™„åŠ å»ºè®®
åœºæ™¯	æ¨èåŠ¨ä½œ
ç»“æœå¯è§†åŒ–	å°†ADSå±‚è¡¨å¯¼å…¥ Superset / FineBI
æ•°æ®é‡å¢å¤§	æ›¿æ¢ä¸º ORC/Parquet æ ¼å¼
è‡ªåŠ¨åŒ–è°ƒåº¦	åæœŸå¯æ¥å…¥ Airflow / Azkaban
å­¦ä¹ æ‹“å±•	å¢åŠ â€œç”¨æˆ·ç•™å­˜åˆ†æâ€â€œå¸–å­ç”Ÿå‘½å‘¨æœŸåˆ†æâ€ç­‰æ¨¡å—
